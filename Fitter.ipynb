{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/cpu:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 12037750176328024897\n",
      ", name: \"/gpu:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 215885414\n",
      "locality {\n",
      "  bus_id: 1\n",
      "}\n",
      "incarnation: 6307635448333367168\n",
      "physical_device_desc: \"device: 0, name: GeForce GTX TITAN X, pci bus id: 0000:01:00.0\"\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Conv2D\n",
    "from keras.preprocessing import sequence\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers import Dense\n",
    "from keras.datasets import imdb\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import numpy\n",
    "\n",
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "left_stay_train = numpy.load('train/leftStay.npy')\n",
    "left_stay2_train = numpy.load('train/leftStay2.npy')\n",
    "right_stay_train = numpy.load('train/rightStay.npy')\n",
    "right_stay2_train = numpy.load('train/rightStay2.npy')\n",
    "matt_right_stay_train = numpy.load('train/mattRightStay.npy')\n",
    "left_back_train = numpy.load('train/leftBack.npy')\n",
    "right_back_train = numpy.load('train/rightBack.npy')\n",
    "n_samples = len(right_back_train)\n",
    "\n",
    "Y_train = []\n",
    "for i in range(n_samples):\n",
    "    Y_train.append([1, 0, 0, 0])\n",
    "for i in range(n_samples):\n",
    "    Y_train.append([0, 1, 0, 0])\n",
    "for i in range(100):\n",
    "    Y_train.append([0, 1, 0, 0])\n",
    "for i in range(n_samples):\n",
    "    Y_train.append([0, 0, 1, 0])\n",
    "for i in range(n_samples):\n",
    "    Y_train.append([0, 0, 0, 1])\n",
    "for i in range(100):\n",
    "    Y_train.append([0, 0, 0, 1])\n",
    "\n",
    "X_train = numpy.concatenate((left_back_train, left_stay_train, left_stay2_train, right_back_train, right_stay_train, right_stay2_train))#, matt_right_stay_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "left_stay_test = numpy.load('test/testLeftStay.npy')\n",
    "right_stay_test = numpy.load('test/testRightStay.npy')\n",
    "left_back_test = numpy.load('test/testLeftBack.npy')\n",
    "right_back_test = numpy.load('test/testRightBack.npy')\n",
    "n_samples = len(right_back_test)\n",
    "\n",
    "Y_test = []\n",
    "for i in range(n_samples):\n",
    "    Y_test.append([1, 0, 0, 0])\n",
    "for i in range(n_samples):\n",
    "    Y_test.append([0, 1, 0, 0])\n",
    "for i in range(n_samples):\n",
    "    Y_test.append([0, 0, 1, 0])\n",
    "for i in range(n_samples):\n",
    "    Y_test.append([0, 0, 0, 1])\n",
    "\n",
    "X_test = numpy.concatenate((left_back_test, left_stay_test, right_back_test, right_stay_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_186 (LSTM)              (None, 48)                10560     \n",
      "_________________________________________________________________\n",
      "dense_181 (Dense)            (None, 4)                 196       \n",
      "=================================================================\n",
      "Total params: 10,756\n",
      "Trainable params: 10,756\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 260 samples, validate on 60 samples\n",
      "Epoch 1/5\n",
      "260/260 [==============================] - 14s - loss: 1.3352 - acc: 0.4000 - val_loss: 1.4116 - val_acc: 0.2500\n",
      "Epoch 2/5\n",
      "260/260 [==============================] - 3s - loss: 1.1812 - acc: 0.4423 - val_loss: 1.6422 - val_acc: 0.2500\n",
      "Epoch 3/5\n",
      "260/260 [==============================] - 3s - loss: 1.0610 - acc: 0.4423 - val_loss: 1.8797 - val_acc: 0.2500\n",
      "Epoch 4/5\n",
      "260/260 [==============================] - 3s - loss: 1.0557 - acc: 0.4423 - val_loss: 1.8810 - val_acc: 0.2500\n",
      "Epoch 5/5\n",
      "260/260 [==============================] - 3s - loss: 1.0530 - acc: 0.4423 - val_loss: 1.8748 - val_acc: 0.2500\n",
      "Overfit Accuracy: 44.23%\n",
      "Test Accuracy: 25.00%\n"
     ]
    }
   ],
   "source": [
    "# create the model\n",
    "model = Sequential()\n",
    "model.add(Convolution2D(32, 5, 5, border_mode='same', input_shape=(200, 6)))\n",
    "model.add(MaxPooling2D(pool_size=(10, 10)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Permute(0, 3, 2, 1))\n",
    "model.add(LSTM(48, dropout=0.1, recurrent_dropout=0.1))\n",
    "model.add(Dense(4, activation='sigmoid'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())\n",
    "\n",
    "model.fit(X_train, Y_train, validation_data=(X_test, Y_test), epochs=5, batch_size=20, shuffle=True)\n",
    "# Final evaluation of the model\n",
    "scores = model.evaluate(X_train, Y_train, verbose=0)\n",
    "print(\"Overfit Accuracy: %.2f%%\" % (scores[1]*100))\n",
    "scores = model.evaluate(X_test, Y_test, verbose=0)\n",
    "print(\"Test Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 75.00%\n"
     ]
    }
   ],
   "source": [
    "# Final evaluation of the model\n",
    "scores = model.evaluate(X_train, Y_train, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "160"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_169 (LSTM)              (None, 100)               42800     \n",
      "_________________________________________________________________\n",
      "dense_164 (Dense)            (None, 4)                 404       \n",
      "=================================================================\n",
      "Total params: 43,204\n",
      "Trainable params: 43,204\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 160 samples, validate on 60 samples\n",
      "Epoch 1/300\n",
      "160/160 [==============================] - 12s - loss: 1.3375 - acc: 0.5187 - val_loss: 1.3988 - val_acc: 0.2500\n",
      "Epoch 2/300\n",
      "160/160 [==============================] - 2s - loss: 1.1134 - acc: 0.7188 - val_loss: 1.6706 - val_acc: 0.2500\n",
      "Epoch 3/300\n",
      "160/160 [==============================] - 2s - loss: 0.9310 - acc: 0.7188 - val_loss: 2.0178 - val_acc: 0.2500\n",
      "Epoch 4/300\n",
      "160/160 [==============================] - 2s - loss: 0.9130 - acc: 0.7188 - val_loss: 1.9486 - val_acc: 0.2500\n",
      "Epoch 5/300\n",
      "160/160 [==============================] - 2s - loss: 0.9063 - acc: 0.7188 - val_loss: 1.8484 - val_acc: 0.2500\n",
      "Epoch 6/300\n",
      "160/160 [==============================] - 2s - loss: 0.9067 - acc: 0.7188 - val_loss: 1.8271 - val_acc: 0.2500\n",
      "Epoch 7/300\n",
      "160/160 [==============================] - 2s - loss: 0.9100 - acc: 0.7188 - val_loss: 1.8401 - val_acc: 0.2500\n",
      "Epoch 8/300\n",
      "160/160 [==============================] - 2s - loss: 0.9077 - acc: 0.7188 - val_loss: 1.8353 - val_acc: 0.2500\n",
      "Epoch 9/300\n",
      "160/160 [==============================] - 2s - loss: 0.8969 - acc: 0.7188 - val_loss: 1.8484 - val_acc: 0.2500\n",
      "Epoch 10/300\n",
      "160/160 [==============================] - 2s - loss: 0.9082 - acc: 0.7188 - val_loss: 1.9107 - val_acc: 0.2500\n",
      "Epoch 11/300\n",
      "160/160 [==============================] - 2s - loss: 0.9095 - acc: 0.7188 - val_loss: 1.9133 - val_acc: 0.2500\n",
      "Epoch 12/300\n",
      "160/160 [==============================] - 2s - loss: 0.9092 - acc: 0.7188 - val_loss: 1.8204 - val_acc: 0.2500\n",
      "Epoch 13/300\n",
      "160/160 [==============================] - 2s - loss: 0.9057 - acc: 0.7188 - val_loss: 1.8168 - val_acc: 0.2500\n",
      "Epoch 14/300\n",
      "160/160 [==============================] - 2s - loss: 0.9045 - acc: 0.7188 - val_loss: 1.8622 - val_acc: 0.2500\n",
      "Epoch 15/300\n",
      "160/160 [==============================] - 2s - loss: 0.9022 - acc: 0.7188 - val_loss: 1.8847 - val_acc: 0.2500\n",
      "Epoch 16/300\n",
      "160/160 [==============================] - 2s - loss: 0.9029 - acc: 0.7188 - val_loss: 1.9002 - val_acc: 0.2500\n",
      "Epoch 17/300\n",
      "160/160 [==============================] - 2s - loss: 0.9051 - acc: 0.7188 - val_loss: 1.8799 - val_acc: 0.2500\n",
      "Epoch 18/300\n",
      "160/160 [==============================] - 2s - loss: 0.9134 - acc: 0.7188 - val_loss: 1.8436 - val_acc: 0.2500\n",
      "Epoch 19/300\n",
      "160/160 [==============================] - 2s - loss: 0.9060 - acc: 0.7188 - val_loss: 1.8315 - val_acc: 0.2500\n",
      "Epoch 20/300\n",
      "160/160 [==============================] - 2s - loss: 0.9021 - acc: 0.7188 - val_loss: 1.8589 - val_acc: 0.2500\n",
      "Epoch 21/300\n",
      "160/160 [==============================] - 2s - loss: 0.9000 - acc: 0.7188 - val_loss: 1.8542 - val_acc: 0.2500\n",
      "Epoch 22/300\n",
      "160/160 [==============================] - 2s - loss: 0.9041 - acc: 0.7188 - val_loss: 1.9004 - val_acc: 0.2500\n",
      "Epoch 23/300\n",
      "160/160 [==============================] - 2s - loss: 0.9086 - acc: 0.7188 - val_loss: 1.8986 - val_acc: 0.2500\n",
      "Epoch 24/300\n",
      "160/160 [==============================] - 2s - loss: 0.9006 - acc: 0.7188 - val_loss: 1.8358 - val_acc: 0.2500\n",
      "Epoch 25/300\n",
      "160/160 [==============================] - 2s - loss: 0.9019 - acc: 0.7188 - val_loss: 1.8026 - val_acc: 0.2500\n",
      "Epoch 26/300\n",
      "160/160 [==============================] - 2s - loss: 0.9054 - acc: 0.7188 - val_loss: 1.8718 - val_acc: 0.2500\n",
      "Epoch 27/300\n",
      "160/160 [==============================] - 2s - loss: 0.9021 - acc: 0.7188 - val_loss: 1.8353 - val_acc: 0.2500\n",
      "Epoch 28/300\n",
      "160/160 [==============================] - 2s - loss: 0.9043 - acc: 0.7188 - val_loss: 1.8765 - val_acc: 0.2500\n",
      "Epoch 29/300\n",
      "160/160 [==============================] - 2s - loss: 0.9030 - acc: 0.7188 - val_loss: 1.8306 - val_acc: 0.2500\n",
      "Epoch 30/300\n",
      "160/160 [==============================] - 2s - loss: 0.9014 - acc: 0.7188 - val_loss: 1.8598 - val_acc: 0.2500\n",
      "Epoch 31/300\n",
      "160/160 [==============================] - 2s - loss: 0.9045 - acc: 0.7188 - val_loss: 1.8704 - val_acc: 0.2500\n",
      "Epoch 32/300\n",
      "160/160 [==============================] - 2s - loss: 0.8995 - acc: 0.7188 - val_loss: 1.8343 - val_acc: 0.2500\n",
      "Epoch 33/300\n",
      "160/160 [==============================] - 2s - loss: 0.9041 - acc: 0.7188 - val_loss: 1.8724 - val_acc: 0.2500\n",
      "Epoch 34/300\n",
      "160/160 [==============================] - 2s - loss: 0.9067 - acc: 0.7188 - val_loss: 1.9022 - val_acc: 0.2500\n",
      "Epoch 35/300\n",
      "160/160 [==============================] - 2s - loss: 0.9026 - acc: 0.7188 - val_loss: 1.8480 - val_acc: 0.2500\n",
      "Epoch 36/300\n",
      "160/160 [==============================] - 2s - loss: 0.9017 - acc: 0.7188 - val_loss: 1.8324 - val_acc: 0.2500\n",
      "Epoch 37/300\n",
      "160/160 [==============================] - 2s - loss: 0.8994 - acc: 0.7188 - val_loss: 1.8448 - val_acc: 0.2500\n",
      "Epoch 38/300\n",
      "160/160 [==============================] - 2s - loss: 0.9034 - acc: 0.7188 - val_loss: 1.8991 - val_acc: 0.2500\n",
      "Epoch 39/300\n",
      "160/160 [==============================] - 2s - loss: 0.8972 - acc: 0.7188 - val_loss: 1.8850 - val_acc: 0.2500\n",
      "Epoch 40/300\n",
      "160/160 [==============================] - 2s - loss: 0.9049 - acc: 0.7188 - val_loss: 1.8352 - val_acc: 0.2500\n",
      "Epoch 41/300\n",
      "160/160 [==============================] - 2s - loss: 0.9044 - acc: 0.7188 - val_loss: 1.8757 - val_acc: 0.2500\n",
      "Epoch 42/300\n",
      "160/160 [==============================] - 2s - loss: 0.8978 - acc: 0.7188 - val_loss: 1.8573 - val_acc: 0.2500\n",
      "Epoch 43/300\n",
      "160/160 [==============================] - 2s - loss: 0.8993 - acc: 0.7188 - val_loss: 1.8422 - val_acc: 0.2500\n",
      "Epoch 44/300\n",
      "160/160 [==============================] - 2s - loss: 0.8925 - acc: 0.7188 - val_loss: 1.8717 - val_acc: 0.2500\n",
      "Epoch 45/300\n",
      "160/160 [==============================] - 2s - loss: 0.8911 - acc: 0.7188 - val_loss: 1.8683 - val_acc: 0.2500\n",
      "Epoch 46/300\n",
      "160/160 [==============================] - 2s - loss: 0.9118 - acc: 0.7188 - val_loss: 1.8880 - val_acc: 0.2500\n",
      "Epoch 47/300\n",
      "160/160 [==============================] - 2s - loss: 0.8948 - acc: 0.7188 - val_loss: 1.7927 - val_acc: 0.2500\n",
      "Epoch 48/300\n",
      "160/160 [==============================] - 2s - loss: 0.8990 - acc: 0.7188 - val_loss: 1.8582 - val_acc: 0.2500\n",
      "Epoch 49/300\n",
      "160/160 [==============================] - 2s - loss: 0.8985 - acc: 0.7188 - val_loss: 1.8216 - val_acc: 0.2500\n",
      "Epoch 50/300\n",
      "160/160 [==============================] - 2s - loss: 0.9010 - acc: 0.7188 - val_loss: 1.7998 - val_acc: 0.2500\n",
      "Epoch 51/300\n",
      "160/160 [==============================] - 2s - loss: 0.9042 - acc: 0.7188 - val_loss: 1.9487 - val_acc: 0.2500\n",
      "Epoch 52/300\n",
      "160/160 [==============================] - 2s - loss: 0.8989 - acc: 0.7188 - val_loss: 1.8534 - val_acc: 0.2500\n",
      "Epoch 53/300\n",
      "160/160 [==============================] - 2s - loss: 0.8999 - acc: 0.7188 - val_loss: 1.7445 - val_acc: 0.2500\n",
      "Epoch 54/300\n",
      "160/160 [==============================] - 2s - loss: 0.8958 - acc: 0.7188 - val_loss: 1.7777 - val_acc: 0.2500\n",
      "Epoch 55/300\n",
      "160/160 [==============================] - 2s - loss: 0.9061 - acc: 0.7188 - val_loss: 1.8032 - val_acc: 0.2500\n",
      "Epoch 56/300\n",
      "160/160 [==============================] - 2s - loss: 0.9048 - acc: 0.7188 - val_loss: 2.0807 - val_acc: 0.2500\n",
      "Epoch 57/300\n",
      "160/160 [==============================] - 2s - loss: 0.9095 - acc: 0.7188 - val_loss: 1.8642 - val_acc: 0.2500\n",
      "Epoch 58/300\n",
      "160/160 [==============================] - 2s - loss: 0.8911 - acc: 0.7188 - val_loss: 1.8118 - val_acc: 0.2500\n",
      "Epoch 59/300\n",
      "160/160 [==============================] - 2s - loss: 0.8955 - acc: 0.7188 - val_loss: 1.8352 - val_acc: 0.2500\n",
      "Epoch 60/300\n",
      "160/160 [==============================] - 2s - loss: 0.8930 - acc: 0.7188 - val_loss: 1.9471 - val_acc: 0.2500\n",
      "Epoch 61/300\n",
      "160/160 [==============================] - 2s - loss: 0.8780 - acc: 0.7188 - val_loss: 1.7401 - val_acc: 0.2500\n",
      "Epoch 62/300\n",
      "160/160 [==============================] - 2s - loss: 0.8984 - acc: 0.7188 - val_loss: 1.8072 - val_acc: 0.2500\n",
      "Epoch 63/300\n",
      "160/160 [==============================] - 2s - loss: 0.9016 - acc: 0.7188 - val_loss: 1.7817 - val_acc: 0.2500\n",
      "Epoch 64/300\n",
      "160/160 [==============================] - 2s - loss: 0.8946 - acc: 0.7188 - val_loss: 1.8755 - val_acc: 0.2500\n",
      "Epoch 65/300\n",
      "160/160 [==============================] - 2s - loss: 0.9019 - acc: 0.7188 - val_loss: 1.9990 - val_acc: 0.2500\n",
      "Epoch 66/300\n",
      "160/160 [==============================] - 2s - loss: 0.9044 - acc: 0.7188 - val_loss: 1.9192 - val_acc: 0.2500\n",
      "Epoch 67/300\n",
      "160/160 [==============================] - 2s - loss: 0.8863 - acc: 0.7188 - val_loss: 1.7592 - val_acc: 0.2500\n",
      "Epoch 68/300\n",
      "160/160 [==============================] - 2s - loss: 0.8999 - acc: 0.7188 - val_loss: 1.8047 - val_acc: 0.2500\n",
      "Epoch 69/300\n",
      "160/160 [==============================] - 2s - loss: 0.8961 - acc: 0.7188 - val_loss: 1.9068 - val_acc: 0.2500\n",
      "Epoch 70/300\n",
      "160/160 [==============================] - 2s - loss: 0.8951 - acc: 0.7188 - val_loss: 1.8897 - val_acc: 0.2500\n",
      "Epoch 71/300\n",
      "160/160 [==============================] - 2s - loss: 0.9022 - acc: 0.7188 - val_loss: 1.8496 - val_acc: 0.2500\n",
      "Epoch 72/300\n",
      "160/160 [==============================] - 2s - loss: 0.8920 - acc: 0.7188 - val_loss: 1.8674 - val_acc: 0.2500\n",
      "Epoch 73/300\n",
      "160/160 [==============================] - 2s - loss: 0.8941 - acc: 0.7188 - val_loss: 1.9087 - val_acc: 0.2500\n",
      "Epoch 74/300\n",
      "160/160 [==============================] - 2s - loss: 0.8967 - acc: 0.7188 - val_loss: 1.8192 - val_acc: 0.2500\n",
      "Epoch 75/300\n",
      "160/160 [==============================] - 2s - loss: 0.8897 - acc: 0.7188 - val_loss: 1.8655 - val_acc: 0.2500\n",
      "Epoch 76/300\n",
      "160/160 [==============================] - 2s - loss: 0.8898 - acc: 0.7188 - val_loss: 1.9285 - val_acc: 0.2500\n",
      "Epoch 77/300\n",
      "160/160 [==============================] - 2s - loss: 0.8797 - acc: 0.7188 - val_loss: 1.9275 - val_acc: 0.2500\n",
      "Epoch 78/300\n",
      "160/160 [==============================] - 2s - loss: 0.9122 - acc: 0.7188 - val_loss: 1.9906 - val_acc: 0.2500\n",
      "Epoch 79/300\n",
      "160/160 [==============================] - 2s - loss: 0.8826 - acc: 0.7188 - val_loss: 1.7411 - val_acc: 0.2500\n",
      "Epoch 80/300\n",
      "160/160 [==============================] - 2s - loss: 0.8934 - acc: 0.7188 - val_loss: 1.9726 - val_acc: 0.2500\n",
      "Epoch 81/300\n",
      "160/160 [==============================] - 2s - loss: 0.8839 - acc: 0.7188 - val_loss: 1.8638 - val_acc: 0.2500\n",
      "Epoch 82/300\n",
      "160/160 [==============================] - 2s - loss: 0.8887 - acc: 0.7188 - val_loss: 1.7846 - val_acc: 0.2500\n",
      "Epoch 83/300\n",
      "160/160 [==============================] - 2s - loss: 0.8929 - acc: 0.7188 - val_loss: 1.8464 - val_acc: 0.2500\n",
      "Epoch 84/300\n",
      "160/160 [==============================] - 2s - loss: 0.9077 - acc: 0.7188 - val_loss: 2.1097 - val_acc: 0.2500\n",
      "Epoch 85/300\n",
      "160/160 [==============================] - 2s - loss: 0.8974 - acc: 0.7188 - val_loss: 1.9605 - val_acc: 0.2500\n",
      "Epoch 86/300\n",
      "160/160 [==============================] - 2s - loss: 0.8866 - acc: 0.7188 - val_loss: 1.8024 - val_acc: 0.2500\n",
      "Epoch 87/300\n",
      "160/160 [==============================] - 2s - loss: 0.8966 - acc: 0.7188 - val_loss: 1.8290 - val_acc: 0.2500\n",
      "Epoch 88/300\n",
      "160/160 [==============================] - 2s - loss: 0.8915 - acc: 0.7188 - val_loss: 1.8124 - val_acc: 0.2500\n",
      "Epoch 89/300\n",
      "160/160 [==============================] - 2s - loss: 0.8845 - acc: 0.7188 - val_loss: 1.8789 - val_acc: 0.2500\n",
      "Epoch 90/300\n",
      "160/160 [==============================] - 2s - loss: 0.8963 - acc: 0.7188 - val_loss: 1.9398 - val_acc: 0.2500\n",
      "Epoch 91/300\n",
      "160/160 [==============================] - 2s - loss: 0.8886 - acc: 0.7188 - val_loss: 1.8922 - val_acc: 0.2500\n",
      "Epoch 92/300\n",
      "160/160 [==============================] - 2s - loss: 0.8986 - acc: 0.7188 - val_loss: 1.8600 - val_acc: 0.2500\n",
      "Epoch 93/300\n",
      "160/160 [==============================] - 2s - loss: 0.8871 - acc: 0.7188 - val_loss: 1.9177 - val_acc: 0.2500\n",
      "Epoch 94/300\n",
      "160/160 [==============================] - 2s - loss: 0.8780 - acc: 0.7188 - val_loss: 1.8447 - val_acc: 0.2500\n",
      "Epoch 95/300\n",
      "160/160 [==============================] - 2s - loss: 0.8823 - acc: 0.7188 - val_loss: 1.9104 - val_acc: 0.2500\n",
      "Epoch 96/300\n",
      "160/160 [==============================] - 2s - loss: 0.8812 - acc: 0.7188 - val_loss: 1.8801 - val_acc: 0.2500\n",
      "Epoch 97/300\n",
      "160/160 [==============================] - 2s - loss: 0.8942 - acc: 0.7188 - val_loss: 1.8301 - val_acc: 0.2500\n",
      "Epoch 98/300\n",
      "160/160 [==============================] - 2s - loss: 0.8949 - acc: 0.7188 - val_loss: 1.9456 - val_acc: 0.2500\n",
      "Epoch 99/300\n",
      "160/160 [==============================] - 2s - loss: 0.8801 - acc: 0.7188 - val_loss: 1.9656 - val_acc: 0.2500\n",
      "Epoch 100/300\n",
      "160/160 [==============================] - 2s - loss: 0.8924 - acc: 0.7188 - val_loss: 1.8327 - val_acc: 0.2500\n",
      "Epoch 101/300\n",
      "160/160 [==============================] - 2s - loss: 0.8788 - acc: 0.7188 - val_loss: 1.8780 - val_acc: 0.2500\n",
      "Epoch 102/300\n",
      "160/160 [==============================] - 2s - loss: 0.8983 - acc: 0.7188 - val_loss: 1.9472 - val_acc: 0.2500\n",
      "Epoch 103/300\n",
      "160/160 [==============================] - 2s - loss: 0.9003 - acc: 0.7188 - val_loss: 1.8103 - val_acc: 0.2500\n",
      "Epoch 104/300\n",
      "160/160 [==============================] - 2s - loss: 0.8975 - acc: 0.7125 - val_loss: 1.8327 - val_acc: 0.2500\n",
      "Epoch 105/300\n",
      "160/160 [==============================] - 2s - loss: 0.8828 - acc: 0.7188 - val_loss: 1.9464 - val_acc: 0.2500\n",
      "Epoch 106/300\n",
      "160/160 [==============================] - 2s - loss: 0.8646 - acc: 0.7188 - val_loss: 1.7427 - val_acc: 0.2500\n",
      "Epoch 107/300\n",
      "160/160 [==============================] - 2s - loss: 0.8470 - acc: 0.7188 - val_loss: 2.1034 - val_acc: 0.2500\n",
      "Epoch 108/300\n",
      "160/160 [==============================] - 2s - loss: 0.8981 - acc: 0.7188 - val_loss: 1.8596 - val_acc: 0.2500\n",
      "Epoch 109/300\n",
      "160/160 [==============================] - 2s - loss: 0.9311 - acc: 0.7188 - val_loss: 1.6946 - val_acc: 0.2500\n",
      "Epoch 110/300\n",
      "160/160 [==============================] - 2s - loss: 0.8809 - acc: 0.7188 - val_loss: 1.9573 - val_acc: 0.2500\n",
      "Epoch 111/300\n",
      "160/160 [==============================] - 2s - loss: 0.8887 - acc: 0.7188 - val_loss: 2.0241 - val_acc: 0.2500\n",
      "Epoch 112/300\n",
      "160/160 [==============================] - 2s - loss: 0.8849 - acc: 0.7188 - val_loss: 1.8891 - val_acc: 0.2500\n",
      "Epoch 113/300\n",
      "160/160 [==============================] - 2s - loss: 0.8859 - acc: 0.7188 - val_loss: 1.7919 - val_acc: 0.2500\n",
      "Epoch 114/300\n",
      "160/160 [==============================] - 2s - loss: 0.8847 - acc: 0.7188 - val_loss: 1.8392 - val_acc: 0.2500\n",
      "Epoch 115/300\n",
      "160/160 [==============================] - 2s - loss: 0.9009 - acc: 0.7188 - val_loss: 1.8901 - val_acc: 0.2500\n",
      "Epoch 116/300\n",
      "160/160 [==============================] - 2s - loss: 0.8775 - acc: 0.7188 - val_loss: 1.9093 - val_acc: 0.2500\n",
      "Epoch 117/300\n",
      "160/160 [==============================] - 2s - loss: 0.8981 - acc: 0.7188 - val_loss: 1.8488 - val_acc: 0.2500\n",
      "Epoch 118/300\n",
      "160/160 [==============================] - 2s - loss: 0.8949 - acc: 0.7188 - val_loss: 1.8610 - val_acc: 0.2500\n",
      "Epoch 119/300\n",
      "160/160 [==============================] - 2s - loss: 0.8797 - acc: 0.7188 - val_loss: 1.9539 - val_acc: 0.2500\n",
      "Epoch 120/300\n",
      "160/160 [==============================] - 2s - loss: 0.8689 - acc: 0.7188 - val_loss: 1.8444 - val_acc: 0.2500\n",
      "Epoch 121/300\n",
      "160/160 [==============================] - 2s - loss: 0.9048 - acc: 0.7188 - val_loss: 1.7973 - val_acc: 0.2500\n",
      "Epoch 122/300\n",
      "160/160 [==============================] - 2s - loss: 0.8789 - acc: 0.7188 - val_loss: 1.9297 - val_acc: 0.2500\n",
      "Epoch 123/300\n",
      "160/160 [==============================] - 2s - loss: 0.8591 - acc: 0.7188 - val_loss: 1.9536 - val_acc: 0.2500\n",
      "Epoch 124/300\n",
      "160/160 [==============================] - 2s - loss: 0.8743 - acc: 0.7188 - val_loss: 1.8598 - val_acc: 0.2500\n",
      "Epoch 125/300\n",
      "160/160 [==============================] - 2s - loss: 0.8545 - acc: 0.7188 - val_loss: 2.1354 - val_acc: 0.2500\n",
      "Epoch 126/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "160/160 [==============================] - 2s - loss: 0.8608 - acc: 0.7188 - val_loss: 1.7803 - val_acc: 0.2500\n",
      "Epoch 127/300\n",
      "160/160 [==============================] - 2s - loss: 0.8916 - acc: 0.7188 - val_loss: 2.0868 - val_acc: 0.2500\n",
      "Epoch 128/300\n",
      "160/160 [==============================] - 2s - loss: 0.8889 - acc: 0.7188 - val_loss: 2.0363 - val_acc: 0.2500\n",
      "Epoch 129/300\n",
      "160/160 [==============================] - 2s - loss: 0.9050 - acc: 0.7188 - val_loss: 1.7823 - val_acc: 0.2500\n",
      "Epoch 130/300\n",
      "160/160 [==============================] - 2s - loss: 0.8695 - acc: 0.7188 - val_loss: 1.8516 - val_acc: 0.2500\n",
      "Epoch 131/300\n",
      "160/160 [==============================] - 2s - loss: 0.8537 - acc: 0.7188 - val_loss: 1.9510 - val_acc: 0.2500\n",
      "Epoch 132/300\n",
      "160/160 [==============================] - 2s - loss: 0.8655 - acc: 0.7188 - val_loss: 2.0954 - val_acc: 0.2500\n",
      "Epoch 133/300\n",
      "160/160 [==============================] - 2s - loss: 0.8646 - acc: 0.7188 - val_loss: 1.9062 - val_acc: 0.2500\n",
      "Epoch 134/300\n",
      "160/160 [==============================] - 2s - loss: 0.8624 - acc: 0.7188 - val_loss: 1.9609 - val_acc: 0.2500\n",
      "Epoch 135/300\n",
      "160/160 [==============================] - 2s - loss: 0.8756 - acc: 0.7188 - val_loss: 2.2202 - val_acc: 0.2500\n",
      "Epoch 136/300\n",
      "160/160 [==============================] - 2s - loss: 0.8567 - acc: 0.7188 - val_loss: 1.8138 - val_acc: 0.2500\n",
      "Epoch 137/300\n",
      "160/160 [==============================] - 2s - loss: 0.8735 - acc: 0.7188 - val_loss: 1.9248 - val_acc: 0.2500\n",
      "Epoch 138/300\n",
      "160/160 [==============================] - 2s - loss: 0.8718 - acc: 0.7125 - val_loss: 2.0765 - val_acc: 0.2500\n",
      "Epoch 139/300\n",
      "160/160 [==============================] - 2s - loss: 0.8568 - acc: 0.7188 - val_loss: 1.9755 - val_acc: 0.2500\n",
      "Epoch 140/300\n",
      "160/160 [==============================] - 2s - loss: 0.8545 - acc: 0.7188 - val_loss: 1.9476 - val_acc: 0.2500\n",
      "Epoch 141/300\n",
      "160/160 [==============================] - 2s - loss: 0.8742 - acc: 0.7188 - val_loss: 1.9418 - val_acc: 0.2500\n",
      "Epoch 142/300\n",
      "160/160 [==============================] - 2s - loss: 0.8833 - acc: 0.7188 - val_loss: 1.5120 - val_acc: 0.2500\n",
      "Epoch 143/300\n",
      "160/160 [==============================] - 2s - loss: 0.9292 - acc: 0.7188 - val_loss: 1.9053 - val_acc: 0.2500\n",
      "Epoch 144/300\n",
      "160/160 [==============================] - 2s - loss: 0.8805 - acc: 0.7188 - val_loss: 2.2886 - val_acc: 0.2500\n",
      "Epoch 145/300\n",
      "160/160 [==============================] - 2s - loss: 0.9057 - acc: 0.7188 - val_loss: 1.9700 - val_acc: 0.2500\n",
      "Epoch 146/300\n",
      "160/160 [==============================] - 2s - loss: 0.8849 - acc: 0.7188 - val_loss: 1.7112 - val_acc: 0.2500\n",
      "Epoch 147/300\n",
      "160/160 [==============================] - 2s - loss: 0.9032 - acc: 0.7188 - val_loss: 1.7784 - val_acc: 0.2500\n",
      "Epoch 148/300\n",
      "160/160 [==============================] - 2s - loss: 0.8830 - acc: 0.7188 - val_loss: 1.8782 - val_acc: 0.2500\n",
      "Epoch 149/300\n",
      "160/160 [==============================] - 2s - loss: 0.8769 - acc: 0.7188 - val_loss: 2.0704 - val_acc: 0.2500\n",
      "Epoch 150/300\n",
      "160/160 [==============================] - 2s - loss: 0.8709 - acc: 0.7188 - val_loss: 1.8699 - val_acc: 0.2500\n",
      "Epoch 151/300\n",
      "160/160 [==============================] - 2s - loss: 0.8789 - acc: 0.7188 - val_loss: 1.7775 - val_acc: 0.2500\n",
      "Epoch 152/300\n",
      "160/160 [==============================] - 2s - loss: 0.8797 - acc: 0.7188 - val_loss: 1.8408 - val_acc: 0.2500\n",
      "Epoch 153/300\n",
      "160/160 [==============================] - 2s - loss: 0.8535 - acc: 0.7188 - val_loss: 1.9024 - val_acc: 0.2500\n",
      "Epoch 154/300\n",
      "160/160 [==============================] - 2s - loss: 0.8504 - acc: 0.7188 - val_loss: 1.8733 - val_acc: 0.2500\n",
      "Epoch 155/300\n",
      "160/160 [==============================] - 2s - loss: 0.8576 - acc: 0.7188 - val_loss: 1.8050 - val_acc: 0.2500\n",
      "Epoch 156/300\n",
      "160/160 [==============================] - 2s - loss: 0.8716 - acc: 0.7188 - val_loss: 2.1119 - val_acc: 0.2500\n",
      "Epoch 157/300\n",
      "160/160 [==============================] - 2s - loss: 0.8546 - acc: 0.7188 - val_loss: 1.8465 - val_acc: 0.2500\n",
      "Epoch 158/300\n",
      "160/160 [==============================] - 2s - loss: 0.8297 - acc: 0.7188 - val_loss: 2.3163 - val_acc: 0.2500\n",
      "Epoch 159/300\n",
      "160/160 [==============================] - 2s - loss: 0.9214 - acc: 0.7188 - val_loss: 2.1311 - val_acc: 0.2500\n",
      "Epoch 160/300\n",
      "160/160 [==============================] - 2s - loss: 0.8829 - acc: 0.7188 - val_loss: 1.8433 - val_acc: 0.2500\n",
      "Epoch 161/300\n",
      "160/160 [==============================] - 2s - loss: 0.8914 - acc: 0.7188 - val_loss: 1.7798 - val_acc: 0.2500\n",
      "Epoch 162/300\n",
      "160/160 [==============================] - 2s - loss: 0.8938 - acc: 0.7188 - val_loss: 1.8920 - val_acc: 0.2500\n",
      "Epoch 163/300\n",
      "160/160 [==============================] - 2s - loss: 0.8698 - acc: 0.7188 - val_loss: 1.8478 - val_acc: 0.2500\n",
      "Epoch 164/300\n",
      "160/160 [==============================] - 2s - loss: 0.8725 - acc: 0.7188 - val_loss: 1.8859 - val_acc: 0.2500\n",
      "Epoch 165/300\n",
      "160/160 [==============================] - 2s - loss: 0.8738 - acc: 0.7188 - val_loss: 1.9120 - val_acc: 0.2500\n",
      "Epoch 166/300\n",
      "160/160 [==============================] - 2s - loss: 0.8641 - acc: 0.7188 - val_loss: 1.9194 - val_acc: 0.2500\n",
      "Epoch 167/300\n",
      "160/160 [==============================] - 2s - loss: 0.8619 - acc: 0.7188 - val_loss: 1.9987 - val_acc: 0.2500\n",
      "Epoch 168/300\n",
      "160/160 [==============================] - 2s - loss: 0.8697 - acc: 0.7188 - val_loss: 1.8153 - val_acc: 0.2500\n",
      "Epoch 169/300\n",
      "160/160 [==============================] - 2s - loss: 0.8284 - acc: 0.7188 - val_loss: 2.1115 - val_acc: 0.2500\n",
      "Epoch 170/300\n",
      "160/160 [==============================] - 2s - loss: 0.8222 - acc: 0.7188 - val_loss: 1.9832 - val_acc: 0.2500\n",
      "Epoch 171/300\n",
      "160/160 [==============================] - 2s - loss: 0.8319 - acc: 0.7188 - val_loss: 2.0258 - val_acc: 0.2500\n",
      "Epoch 172/300\n",
      "160/160 [==============================] - 2s - loss: 0.8081 - acc: 0.7188 - val_loss: 2.2633 - val_acc: 0.2500\n",
      "Epoch 173/300\n",
      "160/160 [==============================] - 2s - loss: 0.8993 - acc: 0.7188 - val_loss: 2.0482 - val_acc: 0.2500\n",
      "Epoch 174/300\n",
      "160/160 [==============================] - 2s - loss: 0.8507 - acc: 0.7188 - val_loss: 1.8311 - val_acc: 0.2500\n",
      "Epoch 175/300\n",
      "160/160 [==============================] - 2s - loss: 0.8930 - acc: 0.7188 - val_loss: 1.9502 - val_acc: 0.2500\n",
      "Epoch 176/300\n",
      "160/160 [==============================] - 2s - loss: 0.8420 - acc: 0.7188 - val_loss: 1.9298 - val_acc: 0.2500\n",
      "Epoch 177/300\n",
      "160/160 [==============================] - 2s - loss: 0.8664 - acc: 0.7188 - val_loss: 2.1724 - val_acc: 0.2500\n",
      "Epoch 178/300\n",
      "160/160 [==============================] - 2s - loss: 0.8963 - acc: 0.7188 - val_loss: 1.9949 - val_acc: 0.2500\n",
      "Epoch 179/300\n",
      "160/160 [==============================] - 2s - loss: 0.8675 - acc: 0.7188 - val_loss: 1.8314 - val_acc: 0.2500\n",
      "Epoch 180/300\n",
      "160/160 [==============================] - 2s - loss: 0.8701 - acc: 0.7188 - val_loss: 1.8240 - val_acc: 0.2500\n",
      "Epoch 181/300\n",
      "160/160 [==============================] - 2s - loss: 0.8931 - acc: 0.7188 - val_loss: 1.9017 - val_acc: 0.2500\n",
      "Epoch 182/300\n",
      "160/160 [==============================] - 2s - loss: 0.8733 - acc: 0.7188 - val_loss: 1.9289 - val_acc: 0.2500\n",
      "Epoch 183/300\n",
      "160/160 [==============================] - 2s - loss: 0.8604 - acc: 0.7188 - val_loss: 1.8417 - val_acc: 0.2500\n",
      "Epoch 184/300\n",
      "160/160 [==============================] - 2s - loss: 0.8306 - acc: 0.7188 - val_loss: 1.9399 - val_acc: 0.2500\n",
      "Epoch 185/300\n",
      "160/160 [==============================] - 2s - loss: 0.8558 - acc: 0.7188 - val_loss: 1.9391 - val_acc: 0.2500\n",
      "Epoch 186/300\n",
      "160/160 [==============================] - 2s - loss: 0.8297 - acc: 0.7188 - val_loss: 1.9068 - val_acc: 0.2500\n",
      "Epoch 187/300\n",
      "160/160 [==============================] - 2s - loss: 0.8270 - acc: 0.7188 - val_loss: 2.0432 - val_acc: 0.2500\n",
      "Epoch 188/300\n",
      "160/160 [==============================] - 2s - loss: 0.8227 - acc: 0.7188 - val_loss: 2.0136 - val_acc: 0.2500\n",
      "Epoch 189/300\n",
      "160/160 [==============================] - 2s - loss: 0.9580 - acc: 0.7188 - val_loss: 1.6138 - val_acc: 0.2500\n",
      "Epoch 190/300\n",
      "160/160 [==============================] - 2s - loss: 0.9012 - acc: 0.7188 - val_loss: 2.4130 - val_acc: 0.2500\n",
      "Epoch 191/300\n",
      "160/160 [==============================] - 2s - loss: 0.9043 - acc: 0.7188 - val_loss: 2.0536 - val_acc: 0.2500\n",
      "Epoch 192/300\n",
      "160/160 [==============================] - 2s - loss: 0.8586 - acc: 0.7188 - val_loss: 1.7735 - val_acc: 0.2500\n",
      "Epoch 193/300\n",
      "160/160 [==============================] - 2s - loss: 0.8778 - acc: 0.7188 - val_loss: 1.7510 - val_acc: 0.2500\n",
      "Epoch 194/300\n",
      "160/160 [==============================] - 2s - loss: 0.8665 - acc: 0.7188 - val_loss: 1.8557 - val_acc: 0.2500\n",
      "Epoch 195/300\n",
      "160/160 [==============================] - 2s - loss: 0.8625 - acc: 0.7188 - val_loss: 1.9158 - val_acc: 0.2500\n",
      "Epoch 196/300\n",
      "160/160 [==============================] - 2s - loss: 0.8559 - acc: 0.7188 - val_loss: 1.9818 - val_acc: 0.2500\n",
      "Epoch 197/300\n",
      "160/160 [==============================] - 2s - loss: 0.8613 - acc: 0.7188 - val_loss: 1.8830 - val_acc: 0.2500\n",
      "Epoch 198/300\n",
      "160/160 [==============================] - 2s - loss: 0.8311 - acc: 0.7188 - val_loss: 1.8779 - val_acc: 0.2500\n",
      "Epoch 199/300\n",
      "160/160 [==============================] - 2s - loss: 0.8707 - acc: 0.7188 - val_loss: 2.0597 - val_acc: 0.2500\n",
      "Epoch 200/300\n",
      "160/160 [==============================] - 2s - loss: 0.8258 - acc: 0.7188 - val_loss: 1.9871 - val_acc: 0.2500\n",
      "Epoch 201/300\n",
      "160/160 [==============================] - 2s - loss: 0.8183 - acc: 0.7188 - val_loss: 1.9640 - val_acc: 0.2500\n",
      "Epoch 202/300\n",
      "160/160 [==============================] - 2s - loss: 0.8532 - acc: 0.7188 - val_loss: 2.2146 - val_acc: 0.2500\n",
      "Epoch 203/300\n",
      "160/160 [==============================] - 2s - loss: 0.8315 - acc: 0.7188 - val_loss: 1.9045 - val_acc: 0.2500\n",
      "Epoch 204/300\n",
      "160/160 [==============================] - 2s - loss: 0.8410 - acc: 0.7188 - val_loss: 1.9548 - val_acc: 0.2500\n",
      "Epoch 205/300\n",
      "160/160 [==============================] - 2s - loss: 0.8829 - acc: 0.7188 - val_loss: 2.0836 - val_acc: 0.2500\n",
      "Epoch 206/300\n",
      "160/160 [==============================] - 2s - loss: 0.8629 - acc: 0.7188 - val_loss: 1.7076 - val_acc: 0.2500\n",
      "Epoch 207/300\n",
      "160/160 [==============================] - 2s - loss: 0.8656 - acc: 0.7188 - val_loss: 2.0806 - val_acc: 0.2500\n",
      "Epoch 208/300\n",
      "160/160 [==============================] - 2s - loss: 0.8567 - acc: 0.7188 - val_loss: 2.1058 - val_acc: 0.2500\n",
      "Epoch 209/300\n",
      "160/160 [==============================] - 2s - loss: 0.8298 - acc: 0.7188 - val_loss: 1.8661 - val_acc: 0.2500\n",
      "Epoch 210/300\n",
      "160/160 [==============================] - 2s - loss: 0.8503 - acc: 0.7188 - val_loss: 1.9248 - val_acc: 0.2500\n",
      "Epoch 211/300\n",
      "160/160 [==============================] - 2s - loss: 0.8049 - acc: 0.7188 - val_loss: 2.2113 - val_acc: 0.2500\n",
      "Epoch 212/300\n",
      "160/160 [==============================] - 2s - loss: 0.9022 - acc: 0.7188 - val_loss: 2.2346 - val_acc: 0.2500\n",
      "Epoch 213/300\n",
      "160/160 [==============================] - 2s - loss: 0.8557 - acc: 0.7188 - val_loss: 1.9122 - val_acc: 0.2500\n",
      "Epoch 214/300\n",
      "160/160 [==============================] - 2s - loss: 0.8545 - acc: 0.7188 - val_loss: 1.7465 - val_acc: 0.2500\n",
      "Epoch 215/300\n",
      "160/160 [==============================] - 2s - loss: 0.8501 - acc: 0.7188 - val_loss: 1.9490 - val_acc: 0.2500\n",
      "Epoch 216/300\n",
      "160/160 [==============================] - 2s - loss: 0.8857 - acc: 0.7188 - val_loss: 2.0069 - val_acc: 0.2500\n",
      "Epoch 217/300\n",
      "160/160 [==============================] - 2s - loss: 0.8216 - acc: 0.7188 - val_loss: 1.8149 - val_acc: 0.2500\n",
      "Epoch 218/300\n",
      "160/160 [==============================] - 2s - loss: 0.8191 - acc: 0.7188 - val_loss: 1.9300 - val_acc: 0.2500\n",
      "Epoch 219/300\n",
      "160/160 [==============================] - 2s - loss: 0.8375 - acc: 0.7188 - val_loss: 2.3774 - val_acc: 0.2500\n",
      "Epoch 220/300\n",
      "160/160 [==============================] - 2s - loss: 0.8961 - acc: 0.7188 - val_loss: 2.1881 - val_acc: 0.2500\n",
      "Epoch 221/300\n",
      "160/160 [==============================] - 2s - loss: 0.8417 - acc: 0.7188 - val_loss: 1.8312 - val_acc: 0.2500\n",
      "Epoch 222/300\n",
      "160/160 [==============================] - 2s - loss: 0.8690 - acc: 0.7188 - val_loss: 1.7136 - val_acc: 0.2500\n",
      "Epoch 223/300\n",
      "160/160 [==============================] - 2s - loss: 0.8486 - acc: 0.7188 - val_loss: 1.7897 - val_acc: 0.2500\n",
      "Epoch 224/300\n",
      "160/160 [==============================] - 2s - loss: 0.9147 - acc: 0.7188 - val_loss: 1.9090 - val_acc: 0.2500\n",
      "Epoch 225/300\n",
      "160/160 [==============================] - 2s - loss: 0.8482 - acc: 0.7188 - val_loss: 2.3307 - val_acc: 0.2500\n",
      "Epoch 226/300\n",
      "160/160 [==============================] - 2s - loss: 0.8517 - acc: 0.7188 - val_loss: 2.1633 - val_acc: 0.2500\n",
      "Epoch 227/300\n",
      "160/160 [==============================] - 2s - loss: 0.8199 - acc: 0.7188 - val_loss: 1.8622 - val_acc: 0.2500\n",
      "Epoch 228/300\n",
      "160/160 [==============================] - 2s - loss: 0.7949 - acc: 0.7188 - val_loss: 1.9650 - val_acc: 0.2500\n",
      "Epoch 229/300\n",
      "160/160 [==============================] - 2s - loss: 0.8095 - acc: 0.7188 - val_loss: 2.2302 - val_acc: 0.2500\n",
      "Epoch 230/300\n",
      "160/160 [==============================] - 2s - loss: 0.8329 - acc: 0.7188 - val_loss: 1.9247 - val_acc: 0.2500\n",
      "Epoch 231/300\n",
      "160/160 [==============================] - 2s - loss: 0.8459 - acc: 0.7188 - val_loss: 2.0693 - val_acc: 0.2500\n",
      "Epoch 232/300\n",
      "160/160 [==============================] - 2s - loss: 0.8264 - acc: 0.7188 - val_loss: 2.1065 - val_acc: 0.2500\n",
      "Epoch 233/300\n",
      "160/160 [==============================] - 2s - loss: 0.8338 - acc: 0.7188 - val_loss: 2.2221 - val_acc: 0.2500\n",
      "Epoch 234/300\n",
      "160/160 [==============================] - 2s - loss: 0.8497 - acc: 0.7188 - val_loss: 1.8381 - val_acc: 0.2500\n",
      "Epoch 235/300\n",
      "160/160 [==============================] - 2s - loss: 0.8473 - acc: 0.7187 - val_loss: 1.7851 - val_acc: 0.2500\n",
      "Epoch 236/300\n",
      "160/160 [==============================] - 2s - loss: 0.8881 - acc: 0.7188 - val_loss: 2.1037 - val_acc: 0.2500\n",
      "Epoch 237/300\n",
      "160/160 [==============================] - 2s - loss: 0.8219 - acc: 0.7188 - val_loss: 1.9940 - val_acc: 0.2500\n",
      "Epoch 238/300\n",
      "160/160 [==============================] - 2s - loss: 0.7942 - acc: 0.7188 - val_loss: 1.9304 - val_acc: 0.2500\n",
      "Epoch 239/300\n",
      "160/160 [==============================] - 2s - loss: 0.8366 - acc: 0.7188 - val_loss: 2.2509 - val_acc: 0.2500\n",
      "Epoch 240/300\n",
      "160/160 [==============================] - 2s - loss: 0.8467 - acc: 0.7188 - val_loss: 2.3017 - val_acc: 0.2500\n",
      "Epoch 241/300\n",
      "160/160 [==============================] - 2s - loss: 0.8744 - acc: 0.7188 - val_loss: 2.1684 - val_acc: 0.2500\n",
      "Epoch 242/300\n",
      "160/160 [==============================] - 2s - loss: 0.8255 - acc: 0.7188 - val_loss: 1.9036 - val_acc: 0.2500\n",
      "Epoch 243/300\n",
      "160/160 [==============================] - 2s - loss: 0.8187 - acc: 0.7188 - val_loss: 1.7866 - val_acc: 0.2500\n",
      "Epoch 244/300\n",
      "160/160 [==============================] - 2s - loss: 0.8356 - acc: 0.7188 - val_loss: 1.8228 - val_acc: 0.2500\n",
      "Epoch 245/300\n",
      "160/160 [==============================] - 2s - loss: 0.9120 - acc: 0.7188 - val_loss: 2.3359 - val_acc: 0.2500\n",
      "Epoch 246/300\n",
      "160/160 [==============================] - 2s - loss: 0.9437 - acc: 0.7188 - val_loss: 2.2283 - val_acc: 0.2500\n",
      "Epoch 247/300\n",
      "160/160 [==============================] - 2s - loss: 0.8964 - acc: 0.7188 - val_loss: 2.0374 - val_acc: 0.2500\n",
      "Epoch 248/300\n",
      "160/160 [==============================] - 2s - loss: 0.8834 - acc: 0.7188 - val_loss: 1.9127 - val_acc: 0.2500\n",
      "Epoch 249/300\n",
      "160/160 [==============================] - 2s - loss: 0.8816 - acc: 0.7188 - val_loss: 1.8550 - val_acc: 0.2500\n",
      "Epoch 250/300\n",
      "160/160 [==============================] - 2s - loss: 0.8845 - acc: 0.7188 - val_loss: 1.8334 - val_acc: 0.2500\n",
      "Epoch 251/300\n",
      "160/160 [==============================] - 2s - loss: 0.8832 - acc: 0.7188 - val_loss: 1.8396 - val_acc: 0.2500\n",
      "Epoch 252/300\n",
      "160/160 [==============================] - 2s - loss: 0.8676 - acc: 0.7188 - val_loss: 1.8568 - val_acc: 0.2500\n",
      "Epoch 253/300\n",
      "160/160 [==============================] - 2s - loss: 0.8780 - acc: 0.7188 - val_loss: 1.8525 - val_acc: 0.2500\n",
      "Epoch 254/300\n",
      "160/160 [==============================] - 2s - loss: 0.8720 - acc: 0.7188 - val_loss: 1.8750 - val_acc: 0.2500\n",
      "Epoch 255/300\n",
      "160/160 [==============================] - 2s - loss: 0.8689 - acc: 0.7188 - val_loss: 1.8745 - val_acc: 0.2500\n",
      "Epoch 256/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "160/160 [==============================] - 2s - loss: 0.8752 - acc: 0.7188 - val_loss: 1.8454 - val_acc: 0.2500\n",
      "Epoch 257/300\n",
      "160/160 [==============================] - 2s - loss: 0.8652 - acc: 0.7188 - val_loss: 1.8722 - val_acc: 0.2500\n",
      "Epoch 258/300\n",
      "160/160 [==============================] - 2s - loss: 0.8604 - acc: 0.7188 - val_loss: 1.8775 - val_acc: 0.2500\n",
      "Epoch 259/300\n",
      "160/160 [==============================] - 2s - loss: 0.8584 - acc: 0.7188 - val_loss: 1.8800 - val_acc: 0.2500\n",
      "Epoch 260/300\n",
      "160/160 [==============================] - 2s - loss: 0.8397 - acc: 0.7188 - val_loss: 1.9568 - val_acc: 0.2500\n",
      "Epoch 261/300\n",
      "160/160 [==============================] - 2s - loss: 0.8428 - acc: 0.7188 - val_loss: 1.8075 - val_acc: 0.2500\n",
      "Epoch 262/300\n",
      "160/160 [==============================] - 2s - loss: 0.8770 - acc: 0.7188 - val_loss: 1.9963 - val_acc: 0.2500\n",
      "Epoch 263/300\n",
      "160/160 [==============================] - 2s - loss: 0.8320 - acc: 0.7188 - val_loss: 2.0501 - val_acc: 0.2500\n",
      "Epoch 264/300\n",
      "160/160 [==============================] - 2s - loss: 0.8400 - acc: 0.7188 - val_loss: 1.9844 - val_acc: 0.2500\n",
      "Epoch 265/300\n",
      "160/160 [==============================] - 2s - loss: 0.8380 - acc: 0.7187 - val_loss: 2.0205 - val_acc: 0.2500\n",
      "Epoch 266/300\n",
      "160/160 [==============================] - 2s - loss: 0.8433 - acc: 0.7188 - val_loss: 2.0107 - val_acc: 0.2500\n",
      "Epoch 267/300\n",
      "160/160 [==============================] - 2s - loss: 0.7868 - acc: 0.7188 - val_loss: 2.0219 - val_acc: 0.2500\n",
      "Epoch 268/300\n",
      "160/160 [==============================] - 2s - loss: 0.8556 - acc: 0.7188 - val_loss: 2.1621 - val_acc: 0.2500\n",
      "Epoch 269/300\n",
      "160/160 [==============================] - 2s - loss: 0.7717 - acc: 0.7188 - val_loss: 2.0529 - val_acc: 0.2500\n",
      "Epoch 270/300\n",
      "160/160 [==============================] - 2s - loss: 0.7785 - acc: 0.7187 - val_loss: 2.0862 - val_acc: 0.2500\n",
      "Epoch 271/300\n",
      "160/160 [==============================] - 2s - loss: 0.8921 - acc: 0.7188 - val_loss: 2.0034 - val_acc: 0.2500\n",
      "Epoch 272/300\n",
      "160/160 [==============================] - 2s - loss: 0.8183 - acc: 0.7188 - val_loss: 2.2352 - val_acc: 0.2500\n",
      "Epoch 273/300\n",
      "160/160 [==============================] - 2s - loss: 0.8555 - acc: 0.7188 - val_loss: 1.8480 - val_acc: 0.2500\n",
      "Epoch 274/300\n",
      "160/160 [==============================] - 2s - loss: 0.8356 - acc: 0.7188 - val_loss: 1.8646 - val_acc: 0.2500\n",
      "Epoch 275/300\n",
      "160/160 [==============================] - 2s - loss: 0.8500 - acc: 0.7188 - val_loss: 2.0548 - val_acc: 0.2500\n",
      "Epoch 276/300\n",
      "160/160 [==============================] - 2s - loss: 0.8551 - acc: 0.7188 - val_loss: 2.0326 - val_acc: 0.2500\n",
      "Epoch 277/300\n",
      "160/160 [==============================] - 2s - loss: 0.8385 - acc: 0.7188 - val_loss: 2.0939 - val_acc: 0.2500\n",
      "Epoch 278/300\n",
      "160/160 [==============================] - 2s - loss: 0.8311 - acc: 0.7188 - val_loss: 1.9404 - val_acc: 0.2500\n",
      "Epoch 279/300\n",
      "160/160 [==============================] - 2s - loss: 0.8396 - acc: 0.7188 - val_loss: 2.1376 - val_acc: 0.2500\n",
      "Epoch 280/300\n",
      "160/160 [==============================] - 2s - loss: 0.8447 - acc: 0.7188 - val_loss: 1.9212 - val_acc: 0.2500\n",
      "Epoch 281/300\n",
      "160/160 [==============================] - 2s - loss: 0.8181 - acc: 0.7188 - val_loss: 2.0757 - val_acc: 0.2500\n",
      "Epoch 282/300\n",
      "160/160 [==============================] - 2s - loss: 0.8154 - acc: 0.7188 - val_loss: 2.0673 - val_acc: 0.2500\n",
      "Epoch 283/300\n",
      "160/160 [==============================] - 2s - loss: 0.8445 - acc: 0.7188 - val_loss: 1.9115 - val_acc: 0.2500\n",
      "Epoch 284/300\n",
      "160/160 [==============================] - 2s - loss: 0.7698 - acc: 0.7188 - val_loss: 1.9489 - val_acc: 0.2500\n",
      "Epoch 285/300\n",
      "160/160 [==============================] - 2s - loss: 0.7854 - acc: 0.7125 - val_loss: 2.3348 - val_acc: 0.2500\n",
      "Epoch 286/300\n",
      "160/160 [==============================] - 2s - loss: 0.8506 - acc: 0.7188 - val_loss: 1.4219 - val_acc: 0.2500\n",
      "Epoch 287/300\n",
      "160/160 [==============================] - 2s - loss: 1.1219 - acc: 0.7188 - val_loss: 1.4386 - val_acc: 0.2500\n",
      "Epoch 288/300\n",
      "160/160 [==============================] - 2s - loss: 1.0518 - acc: 0.7188 - val_loss: 1.5263 - val_acc: 0.2500\n",
      "Epoch 289/300\n",
      "160/160 [==============================] - 2s - loss: 0.9364 - acc: 0.7188 - val_loss: 1.6630 - val_acc: 0.2500\n",
      "Epoch 290/300\n",
      "160/160 [==============================] - 2s - loss: 0.9136 - acc: 0.7187 - val_loss: 1.7988 - val_acc: 0.2500\n",
      "Epoch 291/300\n",
      "160/160 [==============================] - 2s - loss: 0.8846 - acc: 0.7188 - val_loss: 1.9001 - val_acc: 0.2500\n",
      "Epoch 292/300\n",
      "160/160 [==============================] - 2s - loss: 0.8727 - acc: 0.7188 - val_loss: 1.9025 - val_acc: 0.2500\n",
      "Epoch 293/300\n",
      "160/160 [==============================] - 2s - loss: 0.8778 - acc: 0.7188 - val_loss: 1.8825 - val_acc: 0.2500\n",
      "Epoch 294/300\n",
      "160/160 [==============================] - 2s - loss: 0.8690 - acc: 0.7188 - val_loss: 1.8979 - val_acc: 0.2500\n",
      "Epoch 295/300\n",
      "160/160 [==============================] - 2s - loss: 0.8845 - acc: 0.7188 - val_loss: 1.8937 - val_acc: 0.2500\n",
      "Epoch 296/300\n",
      "160/160 [==============================] - 2s - loss: 0.8853 - acc: 0.7188 - val_loss: 1.8770 - val_acc: 0.2500\n",
      "Epoch 297/300\n",
      "160/160 [==============================] - 2s - loss: 0.8804 - acc: 0.7188 - val_loss: 1.8805 - val_acc: 0.2500\n",
      "Epoch 298/300\n",
      "160/160 [==============================] - 2s - loss: 0.8553 - acc: 0.7188 - val_loss: 1.9025 - val_acc: 0.2500\n",
      "Epoch 299/300\n",
      "160/160 [==============================] - 2s - loss: 0.8722 - acc: 0.7188 - val_loss: 1.9180 - val_acc: 0.2500\n",
      "Epoch 300/300\n",
      "160/160 [==============================] - 2s - loss: 0.8624 - acc: 0.7188 - val_loss: 1.9135 - val_acc: 0.2500\n",
      "Overfit Accuracy: 71.88%\n",
      "Test Accuracy: 25.00%\n"
     ]
    }
   ],
   "source": [
    "# create the model\n",
    "model2 = Sequential()\n",
    "model2.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2, input_shape=(200, 6)))\n",
    "model2.add(Dense(4, activation='sigmoid'))\n",
    "\n",
    "model2.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model2.summary())\n",
    "\n",
    "acc = 0.0\n",
    "n_try = 0\n",
    "model2.fit(X_train, Y_train, validation_data=(X_test, Y_test), epochs=300, batch_size=20, shuffle=True)\n",
    "# Final evaluation of the model\n",
    "scores = model2.evaluate(X_train, Y_train, verbose=0)\n",
    "print(\"Overfit Accuracy: %.2f%%\" % (scores[1]*100))\n",
    "scores = model2.evaluate(X_test, Y_test, verbose=0)\n",
    "print(\"Test Accuracy: %.2f%%\" % (scores[1]*100))\n",
    "\n",
    "acc = scores[1]\n",
    "n_try += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1974993d400>]"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\naMwp+IRNWAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1974993d748>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(right_stay_test[0].T[0])\n",
    "plt.plot(left_stay_test[0].T[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  4.83779660e-01,   4.84817273e-01,   4.86633097e-01,\n",
       "         4.88708324e-01,   4.92233158e-01,   4.93789578e-01,\n",
       "         4.93102922e-01,   4.91714351e-01,   4.87014572e-01,\n",
       "         4.81429770e-01,   4.78683146e-01,   4.78881514e-01,\n",
       "         4.82055390e-01,   4.84756237e-01,   4.86007477e-01,\n",
       "         4.85595483e-01,   4.85656519e-01,   4.86266880e-01,\n",
       "         4.85931182e-01,   4.86984054e-01,   4.88998245e-01,\n",
       "         4.91134508e-01,   4.92080568e-01,   4.90417334e-01,\n",
       "         4.87685969e-01,   4.85076677e-01,   4.85122454e-01,\n",
       "         4.87579156e-01,   4.89410239e-01,   4.89913787e-01,\n",
       "         4.89669642e-01,   4.88418402e-01,   4.85961700e-01,\n",
       "         4.80346380e-01,   4.76958877e-01,   4.77401389e-01,\n",
       "         4.81521324e-01,   4.86953536e-01,   4.91042954e-01,\n",
       "         4.91073472e-01,   4.88830396e-01,   4.87045090e-01,\n",
       "         4.86358434e-01,   4.85809110e-01,   4.85458152e-01,\n",
       "         4.87045090e-01,   4.88281071e-01,   4.88494697e-01,\n",
       "         4.85778592e-01,   4.81918059e-01,   4.78286412e-01,\n",
       "         4.77813382e-01,   4.79476616e-01,   4.82009613e-01,\n",
       "         4.84649424e-01,   4.86800946e-01,   4.89929046e-01,\n",
       "         4.95147631e-01,   4.99069200e-01,   5.01709010e-01,\n",
       "         5.03189136e-01,   5.07888914e-01,   5.13138018e-01,\n",
       "         5.20523384e-01,   5.28274968e-01,   5.38284886e-01,\n",
       "         5.51529717e-01,   5.67765316e-01,   5.86533913e-01,\n",
       "         6.07469291e-01,   6.29976349e-01,   6.53734646e-01,\n",
       "         6.77630274e-01,   7.01770047e-01,   7.27176318e-01,\n",
       "         7.56076905e-01,   7.88593881e-01,   8.16517891e-01,\n",
       "         8.45586328e-01,   8.72442206e-01,   9.01937896e-01,\n",
       "         9.27084764e-01,   9.42389563e-01,   9.37506676e-01,\n",
       "         9.22781720e-01,   9.06652934e-01,   8.91500725e-01,\n",
       "         8.73846036e-01,   8.55596246e-01,   8.36507210e-01,\n",
       "         8.11787594e-01,   7.66712444e-01,   7.43274586e-01,\n",
       "         7.18341344e-01,   6.67818723e-01,   6.19752804e-01,\n",
       "         5.71290150e-01,   5.27527276e-01,   4.68764782e-01,\n",
       "         4.00175479e-01,   3.33966583e-01,   2.69817655e-01,\n",
       "         2.06767376e-01,   1.94499123e-01,   1.20752270e-01,\n",
       "         3.63927672e-03,  -7.62951095e-06,  -7.62951095e-06,\n",
       "        -7.62951095e-06,  -7.62951095e-06,  -7.62951095e-06,\n",
       "         1.19859617e-02,   3.98336767e-02,   6.84138247e-02,\n",
       "         8.61753262e-02,   9.96795605e-02,   1.15716793e-01,\n",
       "         1.20340276e-01,   1.25421530e-01,   1.24017700e-01,\n",
       "         1.23361563e-01,   1.27527276e-01,   1.34988937e-01,\n",
       "         1.41031510e-01,   1.36743725e-01,   1.53177691e-01,\n",
       "         1.44220645e-01,   1.36621653e-01,   1.42313268e-01,\n",
       "         1.26169223e-01,   9.93438621e-02,   9.15617609e-02,\n",
       "         6.97718776e-02,   4.87907225e-02,   4.50522621e-02,\n",
       "         5.57030594e-02,   7.40749218e-02,   9.58495460e-02,\n",
       "         1.20782788e-01,   1.48645762e-01,   1.88502327e-01,\n",
       "         2.36995499e-01,   2.90341039e-01,   3.47653925e-01,\n",
       "         3.95719844e-01,   4.46562905e-01,   5.03402762e-01,\n",
       "         5.61860075e-01,   6.21904326e-01,   6.87685969e-01,\n",
       "         7.53543908e-01,   8.11512932e-01,   8.64217594e-01,\n",
       "         9.08377203e-01,   9.50766766e-01,   9.60456245e-01,\n",
       "         9.96269169e-01,   9.99992370e-01,   9.99992370e-01,\n",
       "         9.99992370e-01,   9.95826658e-01,   9.86564431e-01,\n",
       "         9.58899825e-01,   9.34271763e-01,   9.07141222e-01,\n",
       "         8.63912413e-01,   8.19645991e-01,   7.79850462e-01,\n",
       "         7.41992828e-01,   7.13168536e-01,   6.87060349e-01,\n",
       "         6.59716182e-01,   6.32768750e-01,   6.11924926e-01,\n",
       "         5.92225528e-01,   5.76768139e-01,   5.62317845e-01,\n",
       "         5.46051728e-01,   5.24902724e-01,   5.02777142e-01,\n",
       "         4.93575952e-01,   4.85046159e-01,   4.76012818e-01,\n",
       "         4.74258030e-01,   4.76455329e-01,   4.77721828e-01,\n",
       "         4.86175326e-01,   4.92172122e-01,   4.95071336e-01,\n",
       "         5.01892119e-01,   5.00152590e-01,   4.90508888e-01,\n",
       "         4.83916991e-01,   4.84969863e-01,   4.87060349e-01,\n",
       "         4.90890364e-01,   4.91943236e-01,   4.90142672e-01,\n",
       "         4.86266880e-01,   4.82207980e-01])"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "left_back_test[0].T[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
